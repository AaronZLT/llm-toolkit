{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/zhanglongteng/anaconda3/envs/llmtoolkit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-23 16:11:58,615] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[319, 350, 315, 360]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    ")\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "import deepspeed\n",
    "\n",
    "'''\n",
    "param\n",
    "'''\n",
    "\n",
    "# llama2chat = \"/hpc2hdd/home/lzhang330/ssd_workspace/models/llama-2-7b-chat-hf\"\n",
    "# llama2 = \"/hpc2hdd/home/lzhang330/ssd_workspace/models/Llama-2-7b-hf\"\n",
    "llama = \"/mnt/sdb/zhanglongteng/data2/share/llama-1/llama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False, # Fast tokenizer giving issues.\n",
    "    tokenizer_type='llama', # Needed for HF name change\n",
    ")\n",
    "\n",
    "abcd_idx = [\n",
    "    tokenizer(\"A\").input_ids[1],\n",
    "    tokenizer(\"B\").input_ids[1],\n",
    "    tokenizer(\"C\").input_ids[1],\n",
    "    tokenizer(\"D\").input_ids[1],\n",
    "]\n",
    "\n",
    "print(abcd_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 319], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval\n",
    "llama2 = \"/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf\"\n",
    "\n",
    "device:str ='cuda'  # 'cuda' or 'cpu'\n",
    "task_manager = lm_eval.tasks.TaskManager()\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     args.model_name_or_path,\n",
    "#     cache_dir=args.cache_dir,\n",
    "#     load_in_4bit=args.bits == 4,\n",
    "#     load_in_8bit=args.bits == 8,\n",
    "#     device_map=device_map,\n",
    "#     torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n",
    "#     trust_remote_code=args.trust_remote_code,\n",
    "#     use_auth_token=args.use_auth_token\n",
    "# )\n",
    "\n",
    "# results = lm_eval.simple_evaluate( # call simple_evaluate\n",
    "#     model=lm_obj,\n",
    "#     tasks=[\"taskname1\", \"taskname2\"],\n",
    "#     num_fewshot=0,\n",
    "#     task_manager=task_manager,\n",
    "#     ...\n",
    "# )\n",
    "# lm_eval.tasks.initialize_tasks()\n",
    "results = lm_eval.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=f\"pretrained={llama2},\" + \\\n",
    "        f\"tokenizer={llama2}\",\n",
    "    tasks=['mmlu'],\n",
    "    num_fewshot=5,\n",
    "    task_manager=task_manager,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtoolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
