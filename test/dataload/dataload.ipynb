{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/zhanglongteng/anaconda3/envs/llmtoolkit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 15:26:40,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[319, 350, 315, 360]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    ")\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "import deepspeed\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "'''\n",
    "param\n",
    "'''\n",
    "\n",
    "# llama2chat = \"/hpc2hdd/home/lzhang330/ssd_workspace/models/llama-2-7b-chat-hf\"\n",
    "# llama2 = \"/hpc2hdd/home/lzhang330/ssd_workspace/models/Llama-2-7b-hf\"\n",
    "llama = \"/mnt/sdb/zhanglongteng/data2/share/llama-1/llama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False, # Fast tokenizer giving issues.\n",
    "    tokenizer_type='llama', # Needed for HF name change\n",
    ")\n",
    "\n",
    "abcd_idx = [\n",
    "    tokenizer(\"A\").input_ids[1],\n",
    "    tokenizer(\"B\").input_ids[1],\n",
    "    tokenizer(\"C\").input_ids[1],\n",
    "    tokenizer(\"D\").input_ids[1],\n",
    "]\n",
    "\n",
    "print(abcd_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLM(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    source_max_len: int\n",
    "    target_max_len: int\n",
    "    train_on_source: bool\n",
    "    predict_with_generate: bool\n",
    "    hard_padding: bool\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract elements\n",
    "        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n",
    "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
    "        # Tokenize\n",
    "        tokenized_sources_with_prompt = self.tokenizer(\n",
    "            sources,\n",
    "            padding='max_length' if self.hard_padding else False,\n",
    "            max_length=self.source_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        tokenized_targets = self.tokenizer(\n",
    "            targets,\n",
    "            padding='max_length' if self.hard_padding else False,\n",
    "            max_length=self.target_max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        # Build the input and labels for causal LM\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        for tokenized_source, tokenized_target in zip(\n",
    "            tokenized_sources_with_prompt['input_ids'],\n",
    "            tokenized_targets['input_ids']\n",
    "        ):\n",
    "            if not self.predict_with_generate:\n",
    "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
    "                if not self.train_on_source:\n",
    "                    labels.append(\n",
    "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
    "                    )\n",
    "                else:\n",
    "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
    "            else:\n",
    "                input_ids.append(torch.tensor(tokenized_source))\n",
    "        # Apply padding\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
    "\n",
    "        data_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        }\n",
    "        if labels is not None:\n",
    "            data_dict['labels'] = labels\n",
    "        return data_dict\n",
    "\n",
    "def extract_super_natural_instructions_data(examples, extract_reformulations=False):\n",
    "    out = {\n",
    "        'input': [],\n",
    "        'output': [],\n",
    "    }\n",
    "    print_rank_0(examples)\n",
    "    for instance in examples:\n",
    "        out['input'].append(instance['input'])\n",
    "        out['output'].append(instance['output'])\n",
    "    if extract_reformulations:\n",
    "        for example_reformulations in examples['reformulations']:\n",
    "            if example_reformulations is not None:\n",
    "                for instance in example_reformulations:\n",
    "                    out['input'].append(instance['instruction_with_input'])\n",
    "                    out['output'].append(instance['output'])\n",
    "    print_rank_0(out)\n",
    "    return out\n",
    "\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "\n",
    "def local_dataset(dataset_name):\n",
    "    if dataset_name.endswith('.json') or dataset_name.endswith('.jsonl'):\n",
    "        full_dataset = Dataset.from_json(path_or_paths=dataset_name)\n",
    "    elif dataset_name.endswith('.csv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name))\n",
    "    elif dataset_name.endswith('.tsv'):\n",
    "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name, delimiter='\\t'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset format: {dataset_name}\")\n",
    "\n",
    "    split_dataset = full_dataset.train_test_split(test_size=0.1)\n",
    "    return split_dataset\n",
    "\n",
    "def make_data_module(tokenizer: transformers.PreTrainedTokenizer, args) -> Dict:\n",
    "    \"\"\"\n",
    "    Make dataset and collator for supervised fine-tuning.\n",
    "    Datasets are expected to have the following columns: { `input`, `output` }\n",
    "\n",
    "    Available datasets to be selected with `dataset` argument:\n",
    "        - alpaca, 52002 examples\n",
    "        - alpaca cleaned, 51942 examples\n",
    "        - chip2 (OIG), 210289 examples\n",
    "        - self-instruct, 82612 examples\n",
    "        - hh-rlhf (Anthropic), 160800 examples\n",
    "        - longform, 23.7k examples\n",
    "        - oasst1 (OpenAssistant) primary message tree only, 9,846 examples\n",
    "\n",
    "    Coming soon:\n",
    "        - unnatural instructions core, 66010 examples\n",
    "        - unnatural instructions full, 240670 examples\n",
    "        - alpaca-gpt4, 52002 examples\n",
    "        - unnatural-instructions-gpt4, 9000 examples\n",
    "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
    "        - vicuna\n",
    "\n",
    "    \"\"\"\n",
    "    def load_data(dataset_name, data_path=None):\n",
    "        if data_path is None:\n",
    "            if dataset_name == 'alpaca':\n",
    "                return load_dataset(\"tatsu-lab/alpaca\")\n",
    "            elif dataset_name == 'alpaca-dummy':\n",
    "                return load_dataset(\"Lohse/alpaca-dummy\")\n",
    "            elif dataset_name == 'alpaca-clean':\n",
    "                return load_dataset(\"yahma/alpaca-cleaned\")\n",
    "            elif dataset_name == 'flanv2':\n",
    "                return load_dataset(\"conceptofmind/FLAN_2022\")\n",
    "            elif dataset_name == 'chip2':\n",
    "                return load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
    "            elif dataset_name == 'self-instruct':\n",
    "                return load_dataset(\"yizhongw/self_instruct\", name='self_instruct')\n",
    "            elif dataset_name == 'hh-rlhf':\n",
    "                return load_dataset(\"Anthropic/hh-rlhf\")\n",
    "            elif dataset_name == 'longform':\n",
    "                return load_dataset(\"akoksal/LongForm\")\n",
    "            elif dataset_name == 'oasst1':\n",
    "                return load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "            elif dataset_name == 'vicuna':\n",
    "                raise NotImplementedError(\"Vicuna data was not released.\")\n",
    "            else:\n",
    "                if os.path.exists(dataset_name):\n",
    "                    try:\n",
    "                        args.dataset_format = args.dataset_format if args.dataset_format else \"input-output\"\n",
    "                        full_dataset = local_dataset(dataset_name)\n",
    "                        return full_dataset\n",
    "                    except:\n",
    "                        raise ValueError(f\"Error loading dataset from {dataset_name}\")\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Dataset {dataset_name} not implemented yet.\")\n",
    "        else:\n",
    "            if dataset_name in ['alpaca','alpaca-dummy','alpaca-clean','flanv2','hh-rlhf','longform','oasst1']:\n",
    "                return load_dataset('json', data_dir=os.path.join(data_path,dataset_name))\n",
    "            elif dataset_name == 'chip2':\n",
    "                return load_dataset('json', data_dir=os.path.join(data_path,dataset_name), data_files='unified_chip2.jsonl')\n",
    "            elif dataset_name == 'self-instruct':\n",
    "                return load_dataset('json', data_dir=os.path.join(data_path,dataset_name), name='self_instruct')\n",
    "            elif dataset_name == 'super-natural':\n",
    "                return load_dataset('json', data_dir=os.path.join(data_path,dataset_name))\n",
    "            elif dataset_name == 'vicuna':\n",
    "                raise NotImplementedError(\"Vicuna data was not released.\")\n",
    "            else:\n",
    "                if os.path.exists(dataset_name):\n",
    "                    try:\n",
    "                        args.dataset_format = args.dataset_format if args.dataset_format else \"input-output\"\n",
    "                        full_dataset = local_dataset(dataset_name)\n",
    "                        return full_dataset\n",
    "                    except:\n",
    "                        raise ValueError(f\"Error loading dataset from {dataset_name}\")\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Dataset {dataset_name} not implemented yet.\")\n",
    "\n",
    "    def format_dataset(dataset, dataset_format):\n",
    "        if (\n",
    "            dataset_format == 'alpaca' or dataset_format == 'alpaca-clean' or dataset_format == 'alpaca-dummy' or\n",
    "            (dataset_format is None and args.dataset in ['alpaca', 'alpaca-clean', 'alpaca-dummy'])\n",
    "        ):\n",
    "            dataset = dataset.map(extract_alpaca_dataset)\n",
    "        elif dataset_format == 'chip2' or (dataset_format is None and args.dataset == 'chip2'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
    "                'output': x['text'].split('\\n<bot>: ')[1],\n",
    "            })\n",
    "        elif dataset_format == 'self-instruct' or (dataset_format is None and args.dataset == 'self-instruct'):\n",
    "            for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
    "                dataset = dataset.rename_column(old, new)\n",
    "        elif dataset_format == 'hh-rlhf' or (dataset_format is None and args.dataset == 'hh-rlhf'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['chosen']\n",
    "            })\n",
    "        elif dataset_format == 'oasst1' or (dataset_format is None and args.dataset == 'oasst1'):\n",
    "            dataset = dataset.map(lambda x: {\n",
    "                'input': '',\n",
    "                'output': x['text'],\n",
    "            })\n",
    "        elif dataset_format == 'flanv2' or (dataset_format is None and args.dataset == 'flanv2'):\n",
    "            dataset = dataset.map(lambda x: {'input': x['inputs'],'output': x['targets'],})\n",
    "        elif dataset_format =='super-natural' or (dataset_format is None and args.dataset == 'super-natural'):\n",
    "            dataset = dataset.map(remove_columns=['id'])\n",
    "            # dataset = extract_super_natural_instructions_data(dataset)\n",
    "            # dataset = Dataset.from_dict(dataset)\n",
    "        elif dataset_format == 'input-output':\n",
    "            # leave as is\n",
    "            pass\n",
    "        # Remove unused columns.\n",
    "        dataset = dataset.remove_columns(\n",
    "            [col for col in dataset.column_names['train'] if col not in ['input', 'output']]\n",
    "        )\n",
    "        return dataset\n",
    "    \n",
    "    dataset = load_data(args.dataset, args.data_path)\n",
    "    dataset = format_dataset(dataset, args.dataset_format)\n",
    "    \n",
    "    if args.do_eval or args.do_predict:\n",
    "        if 'eval' in dataset:\n",
    "            eval_dataset = dataset['eval']\n",
    "        else:\n",
    "            print_rank_0('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
    "            dataset = dataset[\"train\"].train_test_split(\n",
    "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
    "            )\n",
    "            eval_dataset = dataset['test']\n",
    "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
    "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "        if args.group_by_length:\n",
    "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "    if args.do_train:\n",
    "        train_dataset = dataset['train']\n",
    "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
    "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "        if args.group_by_length:\n",
    "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
    "\n",
    "    data_collator = DataCollatorForCausalLM(\n",
    "        tokenizer=tokenizer,\n",
    "        source_max_len=args.source_max_len,\n",
    "        target_max_len=args.target_max_len,\n",
    "        train_on_source=args.train_on_source,\n",
    "        predict_with_generate=args.predict_with_generate,\n",
    "        hard_padding=args.hard_padding,\n",
    "    )\n",
    "    return dict(\n",
    "        train_dataset=train_dataset if args.do_train else None,\n",
    "        eval_dataset=eval_dataset if args.do_eval else None,\n",
    "        predict_dataset=eval_dataset if args.do_predict else None,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=\"/mnt/sdb/zhanglongteng/llm-toolkit\"\n",
    "\n",
    "llama2=\"/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf\"\n",
    "\n",
    "alpaca = load_dataset('json', data_dir=os.path.join(main_path,\"datasets/alpaca\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52002/52002 [00:03<00:00, 15549.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52002/52002 [00:03<00:00, 14769.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_rm = alpaca.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
    "dataset = alpaca.map(extract_alpaca_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe a time when you had to make a difficult decision.\n",
      "\n",
      "Identify the odd one out.\n",
      "Twitter, Instagram, Telegram\n"
     ]
    }
   ],
   "source": [
    "print(alpaca[\"train\"][\"instruction\"][4])\n",
    "print(alpaca[\"train\"][\"input\"][4])\n",
    "print(alpaca[\"train\"][\"instruction\"][5])\n",
    "print(alpaca[\"train\"][\"input\"][5])\n",
    "# alpaca[\"train\"][\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'instruction', 'input'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe a time when you had to make a difficult decision.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a time when you had to make a difficult decision.\n",
      "\n",
      "### Response: \n",
      "Identify the odd one out.\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the odd one out.\n",
      "\n",
      "### Input:\n",
      "Twitter, Instagram, Telegram\n",
      "\n",
      "### Response: \n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"instruction\"][4])\n",
    "print(dataset[\"train\"][\"input\"][4])\n",
    "print(dataset[\"train\"][\"instruction\"][5])\n",
    "print(dataset[\"train\"][\"input\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a time when you had to make a difficult decision.\n",
      "\n",
      "### Response: \n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the odd one out.\n",
      "\n",
      "### Input:\n",
      "Twitter, Instagram, Telegram\n",
      "\n",
      "### Response: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataset_rm[\"train\"][\"input\"][4])\n",
    "\n",
    "print(dataset_rm[\"train\"][\"input\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_dict2file(dictionary:Dict, filename):\n",
    "    lock = threading.Lock()\n",
    "    lock.acquire()\n",
    "    with open(filename, 'a') as json_file:\n",
    "        try:\n",
    "            json.dump(dictionary, json_file, indent=4)\n",
    "            json_file.write(\"\\n\")\n",
    "        finally:\n",
    "            lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 319], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval\n",
    "llama2 = \"/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf\"\n",
    "\n",
    "device:str ='cuda'  # 'cuda' or 'cpu'\n",
    "task_manager = lm_eval.tasks.TaskManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08:16:12:54,635 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-08:16:12:54,636 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': '/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf', 'tokenizer': '/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf'}\n",
      "2024-04-08:16:12:54,638 INFO     [huggingface.py:163] Using device 'cuda'\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.95s/it]\n",
      "/mnt/sdb/zhanglongteng/anaconda3/envs/llmtoolkit/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-04-08:16:18:25,270 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
      "2024-04-08:16:18:25,272 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
      "2024-04-08:16:18:25,273 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
      "2024-04-08:16:18:25,273 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
      "2024-04-08:16:18:25,274 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
      "2024-04-08:16:18:25,275 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
      "2024-04-08:16:18:25,275 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
      "2024-04-08:16:18:25,276 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
      "2024-04-08:16:18:25,276 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
      "2024-04-08:16:18:25,277 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
      "2024-04-08:16:18:25,278 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
      "2024-04-08:16:18:25,278 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
      "2024-04-08:16:18:25,279 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
      "2024-04-08:16:18:25,280 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
      "2024-04-08:16:18:25,280 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
      "2024-04-08:16:18:25,281 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
      "2024-04-08:16:18:25,281 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
      "2024-04-08:16:18:25,281 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
      "2024-04-08:16:18:25,282 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
      "2024-04-08:16:18:25,282 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
      "2024-04-08:16:18:25,282 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
      "2024-04-08:16:18:25,283 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
      "2024-04-08:16:18:25,283 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
      "2024-04-08:16:18:25,283 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
      "2024-04-08:16:18:25,284 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
      "2024-04-08:16:18:25,284 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
      "2024-04-08:16:18:25,285 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_management from None to 5\n",
      "2024-04-08:16:18:25,285 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
      "2024-04-08:16:18:25,285 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
      "2024-04-08:16:18:25,286 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_virology from None to 5\n",
      "2024-04-08:16:18:25,286 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
      "2024-04-08:16:18:25,286 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
      "2024-04-08:16:18:25,287 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
      "2024-04-08:16:18:25,287 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
      "2024-04-08:16:18:25,287 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
      "2024-04-08:16:18:25,288 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
      "2024-04-08:16:18:25,288 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
      "2024-04-08:16:18:25,288 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
      "2024-04-08:16:18:25,289 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
      "2024-04-08:16:18:25,289 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
      "2024-04-08:16:18:25,289 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
      "2024-04-08:16:18:25,290 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
      "2024-04-08:16:18:25,290 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
      "2024-04-08:16:18:25,291 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
      "2024-04-08:16:18:25,291 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
      "2024-04-08:16:18:25,291 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
      "2024-04-08:16:18:25,292 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
      "2024-04-08:16:18:25,292 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
      "2024-04-08:16:18:25,292 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
      "2024-04-08:16:18:25,293 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
      "2024-04-08:16:18:25,293 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
      "2024-04-08:16:18:25,294 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
      "2024-04-08:16:18:25,294 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
      "2024-04-08:16:18:25,295 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
      "2024-04-08:16:18:25,295 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
      "2024-04-08:16:18:25,295 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
      "2024-04-08:16:18:25,296 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
      "2024-04-08:16:18:25,304 INFO     [task.py:395] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:02<00:00, 63.42it/s]\n",
      "2024-04-08:16:18:28,016 INFO     [task.py:395] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:05<00:00, 66.10it/s]\n",
      "2024-04-08:16:18:33,270 INFO     [task.py:395] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:01<00:00, 62.79it/s]\n",
      "2024-04-08:16:18:34,999 INFO     [task.py:395] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:03<00:00, 65.73it/s]\n",
      "2024-04-08:16:18:38,117 INFO     [task.py:395] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:03<00:00, 65.64it/s]\n",
      "2024-04-08:16:18:41,743 INFO     [task.py:395] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:04<00:00, 65.10it/s]\n",
      "2024-04-08:16:18:46,539 INFO     [task.py:395] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:24<00:00, 63.02it/s]\n",
      "2024-04-08:16:19:10,968 INFO     [task.py:395] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:05<00:00, 63.13it/s]\n",
      "2024-04-08:16:19:16,137 INFO     [task.py:395] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:13<00:00, 66.74it/s]\n",
      "2024-04-08:16:19:29,593 INFO     [task.py:395] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:02<00:00, 66.70it/s]\n",
      "2024-04-08:16:19:32,080 INFO     [task.py:395] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:01<00:00, 67.38it/s]\n",
      "2024-04-08:16:19:33,959 INFO     [task.py:395] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:01<00:00, 66.49it/s]\n",
      "2024-04-08:16:19:35,787 INFO     [task.py:395] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:02<00:00, 67.62it/s]\n",
      "2024-04-08:16:19:38,208 INFO     [task.py:395] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 68.46it/s]\n",
      "2024-04-08:16:19:39,676 INFO     [task.py:395] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:01<00:00, 66.89it/s]\n",
      "2024-04-08:16:19:41,643 INFO     [task.py:395] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:03<00:00, 67.65it/s]\n",
      "2024-04-08:16:19:45,279 INFO     [task.py:395] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:03<00:00, 66.72it/s]\n",
      "2024-04-08:16:19:48,307 INFO     [task.py:395] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:03<00:00, 63.02it/s]\n",
      "2024-04-08:16:19:52,098 INFO     [task.py:395] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:03<00:00, 64.44it/s]\n",
      "2024-04-08:16:19:55,184 INFO     [task.py:395] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:01<00:00, 68.39it/s]\n",
      "2024-04-08:16:19:56,859 INFO     [task.py:395] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:08<00:00, 63.80it/s]\n",
      "2024-04-08:16:20:05,429 INFO     [task.py:395] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:06<00:00, 63.46it/s]\n",
      "2024-04-08:16:20:11,597 INFO     [task.py:395] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:02<00:00, 50.71it/s]\n",
      "2024-04-08:16:20:13,780 INFO     [task.py:395] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:09<00:00, 66.90it/s]\n",
      "2024-04-08:16:20:22,971 INFO     [task.py:395] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:02<00:00, 68.91it/s]\n",
      "2024-04-08:16:20:25,785 INFO     [task.py:395] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:04<00:00, 67.66it/s]\n",
      "2024-04-08:16:20:30,324 INFO     [task.py:395] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:01<00:00, 67.24it/s]\n",
      "2024-04-08:16:20:31,864 INFO     [task.py:395] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.98it/s]\n",
      "2024-04-08:16:20:33,344 INFO     [task.py:395] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:11<00:00, 65.79it/s]\n",
      "2024-04-08:16:20:45,289 INFO     [task.py:395] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:02<00:00, 64.73it/s]\n",
      "2024-04-08:16:20:47,866 INFO     [task.py:395] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:03<00:00, 62.64it/s]\n",
      "2024-04-08:16:20:51,616 INFO     [task.py:395] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:03<00:00, 66.35it/s]\n",
      "2024-04-08:16:20:55,625 INFO     [task.py:395] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:03<00:00, 68.53it/s]\n",
      "2024-04-08:16:20:58,893 INFO     [task.py:395] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:04<00:00, 68.62it/s]\n",
      "2024-04-08:16:21:03,020 INFO     [task.py:395] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:02<00:00, 68.55it/s]\n",
      "2024-04-08:16:21:05,557 INFO     [task.py:395] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 65.64it/s]\n",
      "2024-04-08:16:21:07,088 INFO     [task.py:395] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 68.47it/s]\n",
      "2024-04-08:16:21:08,557 INFO     [task.py:395] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:03<00:00, 68.44it/s]\n",
      "2024-04-08:16:21:12,550 INFO     [task.py:395] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:02<00:00, 68.11it/s]\n",
      "2024-04-08:16:21:14,689 INFO     [task.py:395] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:02<00:00, 68.76it/s]\n",
      "2024-04-08:16:21:16,912 INFO     [task.py:395] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 68.25it/s]\n",
      "2024-04-08:16:21:18,385 INFO     [task.py:395] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:02<00:00, 68.12it/s]\n",
      "2024-04-08:16:21:21,378 INFO     [task.py:395] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.58it/s]\n",
      "2024-04-08:16:21:22,865 INFO     [task.py:395] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:04<00:00, 66.03it/s]\n",
      "2024-04-08:16:21:26,970 INFO     [task.py:395] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:02<00:00, 67.22it/s]\n",
      "2024-04-08:16:21:29,123 INFO     [task.py:395] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:01<00:00, 69.30it/s]\n",
      "2024-04-08:16:21:31,079 INFO     [task.py:395] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:01<00:00, 68.48it/s]\n",
      "2024-04-08:16:21:32,576 INFO     [task.py:395] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:02<00:00, 67.54it/s]\n",
      "2024-04-08:16:21:34,822 INFO     [task.py:395] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:03<00:00, 68.28it/s]\n",
      "2024-04-08:16:21:37,999 INFO     [task.py:395] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:03<00:00, 64.50it/s]\n",
      "2024-04-08:16:21:41,656 INFO     [task.py:395] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.98it/s]\n",
      "2024-04-08:16:21:43,135 INFO     [task.py:395] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:01<00:00, 68.04it/s]\n",
      "2024-04-08:16:21:44,789 INFO     [task.py:395] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:05<00:00, 66.86it/s]\n",
      "2024-04-08:16:21:50,463 INFO     [task.py:395] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 66.58it/s]\n",
      "2024-04-08:16:21:51,973 INFO     [task.py:395] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:04<00:00, 64.79it/s]\n",
      "2024-04-08:16:21:56,775 INFO     [task.py:395] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 69.68it/s]\n",
      "2024-04-08:16:21:58,220 INFO     [task.py:395] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 68.56it/s]\n",
      "2024-04-08:16:21:59,685 INFO     [evaluator.py:379] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 56168/56168 [36:42<00:00, 25.50it/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10:12:23:54,377 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-10:12:23:54,379 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': '/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf', 'tokenizer': '/mnt/sdb/zhanglongteng/data2/share/zhanglongteng_A6000/Llama-2-7b-hf'}\n",
      "2024-04-10:12:23:54,386 INFO     [huggingface.py:163] Using device 'cuda'\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]\n",
      "2024-04-10:12:24:12,094 WARNING  [evaluator.py:239] Overwriting default num_fewshot of gsm8k from 5 to 5\n",
      "2024-04-10:12:24:12,098 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
      "100%|██████████| 1319/1319 [00:08<00:00, 156.46it/s]\n",
      "2024-04-10:12:24:20,561 INFO     [evaluator.py:379] Running generate_until requests\n",
      "Running generate_until requests:   0%|          | 0/1319 [00:00<?, ?it/s]/mnt/sdb/zhanglongteng/anaconda3/envs/llmtoolkit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/zhanglongteng/anaconda3/envs/llmtoolkit/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Running generate_until requests:   1%|▏         | 17/1319 [02:05<2:11:03,  6.04s/it]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmtoolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
